package cmd

import (
	"context"
	"encoding/json"
	"fmt"
	"os"
	"strings"
	"time"

	"github.com/fatih/color"
	"github.com/spf13/cobra"
	"recon-toolkit/pkg/crawler"
)

var (
	crawlMaxDepth   int
	crawlMaxPages   int
	crawlTimeout    int
	crawlAggressive bool
	crawlFuzzing    bool
	crawlOutput     string
)

var crawlerCmd = &cobra.Command{
	Use:   "crawl",
	Short: "ğŸ•·ï¸ Advanced smart web crawler and fuzzer",
	Long: `ğŸ•·ï¸ SMART WEB CRAWLER - AI-Powered Endpoint Discovery

Advanced web application crawler with intelligent discovery capabilities:

ğŸ§  INTELLIGENT FEATURES:
  â€¢ Technology stack detection and adaptation
  â€¢ Context-aware endpoint classification
  â€¢ Hidden parameter discovery
  â€¢ Form analysis and classification
  â€¢ JavaScript endpoint extraction

ğŸ¯ DISCOVERY CAPABILITIES:
  â€¢ Admin panel detection
  â€¢ API endpoint enumeration
  â€¢ Upload functionality discovery
  â€¢ Configuration file hunting
  â€¢ Backup file detection
  â€¢ Debug endpoint identification

âš¡ FUZZING ENGINE:
  â€¢ Parameter-based vulnerability testing
  â€¢ XSS payload injection
  â€¢ SQL injection detection
  â€¢ Command injection testing
  â€¢ Local file inclusion probes
  â€¢ Server-side template injection

ğŸ” SMART CLASSIFICATION:
  â€¢ Login forms and authentication
  â€¢ File upload endpoints
  â€¢ API and AJAX endpoints
  â€¢ Administrative interfaces
  â€¢ Configuration panels
  â€¢ Debug and testing pages

Examples:
  recon-toolkit crawl -t https://target.com --aggressive --fuzzing
  recon-toolkit crawl -t https://target.com --max-depth 10 --max-pages 5000
  recon-toolkit crawl -t https://target.com --output endpoints.json`,

	RunE: func(cmd *cobra.Command, args []string) error {
		if target == "" {
			return fmt.Errorf("target is required")
		}

		ctx := context.Background()

		if !silent {
			color.Red("ğŸ•·ï¸ SMART WEB CRAWLER ACTIVATED")
			color.Yellow("Target: %s", target)
			color.Yellow("Max Depth: %d", crawlMaxDepth)
			color.Yellow("Max Pages: %d", crawlMaxPages)
			color.Yellow("Timeout: %ds", crawlTimeout)
			
			if crawlAggressive {
				color.Red("ğŸš¨ AGGRESSIVE MODE")
			}
			if crawlFuzzing {
				color.Red("ğŸ¯ FUZZING ENABLED")
			}
		}

		// Configure crawler
		options := []func(*crawler.SmartCrawler){
			crawler.WithMaxDepth(crawlMaxDepth),
		}

		if crawlAggressive {
			options = append(options, crawler.WithAggressive())
		}

		if crawlFuzzing {
			options = append(options, crawler.WithFuzzing())
		}

		// Create crawler
		smartCrawler := crawler.NewSmartCrawler(target, options...)

		// Execute crawl
		if !silent {
			color.Cyan("ğŸš€ Starting intelligent web crawl...")
			fmt.Println()
		}

		start := time.Now()
		endpoints, err := smartCrawler.Crawl(ctx)
		duration := time.Since(start)

		if err != nil {
			color.Red("âŒ Crawl failed: %v", err)
			return err
		}

		// Display results
		if !silent {
			displayCrawlerResults(smartCrawler, endpoints, duration)
		}

		// Save results
		if crawlOutput != "" {
			err = saveCrawlerResults(endpoints, crawlOutput)
			if err != nil {
				color.Red("Failed to save results: %v", err)
			} else {
				color.Green("ğŸ“Š Results saved: %s", crawlOutput)
			}
		}

		return nil
	},
}

func init() {
	rootCmd.AddCommand(crawlerCmd)

	crawlerCmd.Flags().IntVar(&crawlMaxDepth, "max-depth", 5, "Maximum crawl depth")
	crawlerCmd.Flags().IntVar(&crawlMaxPages, "max-pages", 1000, "Maximum pages to crawl")
	crawlerCmd.Flags().IntVar(&crawlTimeout, "timeout", 10, "Request timeout in seconds")
	crawlerCmd.Flags().BoolVar(&crawlAggressive, "aggressive", false, "Enable aggressive crawling")
	crawlerCmd.Flags().BoolVar(&crawlFuzzing, "fuzzing", true, "Enable vulnerability fuzzing")
	crawlerCmd.Flags().StringVar(&crawlOutput, "output", "", "Output file for results")
}

func displayCrawlerResults(sc *crawler.SmartCrawler, endpoints []*crawler.Endpoint, duration time.Duration) {
	stats := sc.GetStats()
	
	color.Cyan("\nğŸ•·ï¸ SMART CRAWLER RESULTS")
	color.Cyan("=" + strings.Repeat("=", 40))
	
	color.White("Duration: %v", duration)
	color.White("Total Endpoints: %d", stats["total_endpoints"])
	color.White("Pages Visited: %d", stats["visited_pages"])
	color.White("Vulnerable Endpoints: %d", stats["vulnerable_endpoints"])
	
	// Technology detection
	if techs, ok := stats["technologies"].(map[string]bool); ok && len(techs) > 0 {
		color.Cyan("\nğŸ”§ DETECTED TECHNOLOGIES:")
		for tech := range techs {
			color.Green("  âœ… %s", tech)
		}
	}
	
	// Endpoint classification
	if types, ok := stats["endpoint_types"].(map[crawler.EndpointType]int); ok {
		color.Cyan("\nğŸ“Š ENDPOINT BREAKDOWN:")
		for endpointType, count := range types {
			emoji := getEndpointEmoji(endpointType)
			color.White("%s %s: %d", emoji, endpointType, count)
		}
	}
	
	// Interesting findings
	color.Cyan("\nğŸ¯ INTERESTING FINDINGS:")
	adminCount := 0
	loginCount := 0
	uploadCount := 0
	vulnerableCount := 0
	
	for _, endpoint := range endpoints {
		switch endpoint.Type {
		case crawler.TypeAdmin:
			adminCount++
		case crawler.TypeLogin:
			loginCount++
		case crawler.TypeUpload:
			uploadCount++
		}
		if endpoint.Vulnerable {
			vulnerableCount++
		}
	}
	
	if adminCount > 0 {
		color.Red("ğŸ”‘ Admin panels found: %d", adminCount)
	}
	if loginCount > 0 {
		color.Yellow("ğŸ” Login forms found: %d", loginCount)
	}
	if uploadCount > 0 {
		color.Yellow("ğŸ“¤ Upload endpoints found: %d", uploadCount)
	}
	if vulnerableCount > 0 {
		color.Red("ğŸ’€ Potentially vulnerable: %d", vulnerableCount)
	}
	
	// Show top vulnerable endpoints
	vulnerableEndpoints := getVulnerableEndpoints(endpoints)
	if len(vulnerableEndpoints) > 0 {
		color.Red("\nğŸ’€ VULNERABLE ENDPOINTS:")
		for i, endpoint := range vulnerableEndpoints {
			if i >= 10 { // Show max 10
				break
			}
			color.Red("  ğŸ¯ %s (%s)", endpoint.URL, endpoint.Type)
			for _, payload := range endpoint.Payloads {
				color.Red("    ğŸ’¥ %s", payload)
			}
		}
	}
	
	// Cynical assessment
	color.Cyan("\nğŸ­ CYNICAL ASSESSMENT:")
	totalEndpoints := len(endpoints)
	
	if vulnerableCount > 10 {
		color.Red("ğŸ–• This webapp is more fucked than a pornstar in a gangbang!")
		color.Red("ğŸ’€ Developer clearly learned security from YouTube tutorials")
	} else if vulnerableCount > 5 {
		color.Yellow("ğŸª Decent amount of holes to exploit - not bad!")
		color.Yellow("ğŸ¤¡ Security team needs to step up their game")
	} else if vulnerableCount > 0 {
		color.Green("ğŸ›¡ï¸ Some vulnerabilities found, could be worse")
		color.Green("ğŸ¯ Not terrible, but room for improvement")
	} else if totalEndpoints > 50 {
		color.Blue("ğŸ‘‘ Impressive security posture - well defended!")
		color.Blue("ğŸ¥· Respect to the development team")
	} else {
		color.Blue("ğŸ” Limited attack surface discovered")
		color.Blue("ğŸ¤” Either well-secured or hiding something...")
	}
}

func getEndpointEmoji(endpointType crawler.EndpointType) string {
	emojis := map[crawler.EndpointType]string{
		crawler.TypeForm:   "ğŸ“",
		crawler.TypeAPI:    "ğŸ”Œ",
		crawler.TypeAdmin:  "ğŸ”‘",
		crawler.TypeUpload: "ğŸ“¤",
		crawler.TypeLogin:  "ğŸ”",
		crawler.TypeConfig: "âš™ï¸",
		crawler.TypeBackup: "ğŸ’¾",
		crawler.TypeDebug:  "ğŸ›",
		crawler.TypeHidden: "ğŸ‘»",
		crawler.TypeAjax:   "âš¡",
	}
	
	if emoji, exists := emojis[endpointType]; exists {
		return emoji
	}
	return "ğŸ­"
}

func getVulnerableEndpoints(endpoints []*crawler.Endpoint) []*crawler.Endpoint {
	var vulnerable []*crawler.Endpoint
	
	for _, endpoint := range endpoints {
		if endpoint.Vulnerable {
			vulnerable = append(vulnerable, endpoint)
		}
	}
	
	return vulnerable
}

func saveCrawlerResults(endpoints []*crawler.Endpoint, filename string) error {
	data, err := json.MarshalIndent(endpoints, "", "  ")
	if err != nil {
		return err
	}
	
	return os.WriteFile(filename, data, 0644)
}